{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Import dependencies for data manipulation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../../data/ava_st1_ns4_56.pkl')\n",
    "df.head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish inputs and output columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop('energy', axis=1)\n",
    "y = df['energy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I want to look at the data and do some cleanup if we have any outliers that might skew evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the y values to see the distribution and check for outliers\n",
    "\n",
    "plt.hist(y, bins=100)\n",
    "\n",
    "plt.axvline(y.mean(), color='r', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(y.median(), color='g', linestyle='dashed', linewidth=1)\n",
    "\n",
    "print('mean: ', y.mean())\n",
    "print('median: ', y.median())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the histogram, our data appears to bi-modal. This will impact which evaluation model we use because we will need to make sure that error measurements are not skewed by the bi-modal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Even though the distribution is skewed, there don't appear to be any outliers, but let's check anyway\n",
    "\n",
    "# Identify the outliers using the IQR method\n",
    "\n",
    "q1 = y.quantile(0.25)\n",
    "q3 = y.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - (1.5*iqr)\n",
    "upper_bound = q3 + (1.5*iqr)\n",
    "\n",
    "# Identify the number of outliers\n",
    "\n",
    "print('number of outliers: ', len(y[y > upper_bound]) + len(y[y < lower_bound]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since the number of outliers is 0, we can proceed with the model.\n",
    "However, we will have to consider the skewness of the data when we evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import sklearn dependencies for standardization and train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize the data and scale it for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('STscaler', StandardScaler(), make_column_selector(dtype_include=np.float64)),\n",
    "        ('MMscaler', MinMaxScaler(), make_column_selector(dtype_include=np.int64))\n",
    "    ],\n",
    "    remainder = 'passthrough'\n",
    ")\n",
    "\n",
    "preprocessor.fit(x)\n",
    "\n",
    "X =  preprocessor.transform(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now do Holdout Method\n",
    "#### We will use holdout at first to get a baseline accuracy score and show improvements with cross validation and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import model from sci-kit learn and fit the data and then predict the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Dummy regressor to see if the model is better than a trivial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_regressor = DummyRegressor(strategy='mean')\n",
    "dummy_regressor.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies for evaluation metrics and calculate mean squared error, r2 score, mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "my_scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_values = [\"MSE\"]\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_dummy = mean_squared_error(y_test, y_pred_dummy)\n",
    "print(\"The model's mse: \", mse)\n",
    "print(\"The dummy's mse: \", mse_dummy)\n",
    "print(\"Relative error: \", mse/mse_dummy)\n",
    "\n",
    "mse_values.append(mse)\n",
    "mse_values.append(mse_dummy)\n",
    "\n",
    "my_scores.append(mse_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_values = [\"R2\"]\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_dummy = r2_score(y_test, y_pred_dummy)\n",
    "print(\"The model's r2: \", r2)   \n",
    "print(\"The dummy's r2: \", r2_dummy)\n",
    "\n",
    "r2_values.append(r2)\n",
    "r2_values.append(r2_dummy)\n",
    "\n",
    "my_scores.append(r2_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_values = [\"MAE\"]\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mae_dummy = mean_absolute_error(y_test, y_pred_dummy)\n",
    "print(\"The model's mae: \", mae)\n",
    "print(\"The dummy's mae: \", mae_dummy)\n",
    "print(\"Relative error: \", mae/mae_dummy)\n",
    "\n",
    "mae_values.append(mae)\n",
    "mae_values.append(mae_dummy)\n",
    "\n",
    "my_scores.append(mae_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I want to visualize these scores and determine how we'll compare it to other more sophisticated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Evaluation method':<25} {'Score':<20} {'Dummy Score':<20} {'Ratio (Score/Dummy Score)':<20}\")\n",
    "\n",
    "for item in my_scores:\n",
    "    ratio = item[1]/item[2] if item[2] != 0 else 0\n",
    "    print(f\"{item[0]:<25} {item[1]:<20.3f} {item[2]:<20.3f} {ratio:<10.3f}\")\n",
    "    \n",
    "    # print the ratio of the mse score over the mean value of y_test\n",
    "print(f\"Ratio of mae over mean value of y: {(mae)/np.mean(y):.3f}\")\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the bi-modal distribution of the data, we will need to consider the mean absolute error as the primary evaluation metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's also look at the residuals to see if there are any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred - y_test)\n",
    "# plt.scatter(y_test, y_pred_dummy - y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the residuals are slightly homoscedastic, but there is a slight decreasing trend. This is likely due to the bi-modal distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That was the Holdout Method, now do K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pt.2 Hyper parameter tuning with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2510567.60730594 -2518105.26484018 -2412715.         -2413968.48858447\n",
      " -2513546.93150685]\n",
      "Average MAE scores without HPO:  2473780.658447488\n",
      "Average MAE scores without HPO as a percentage of the mean y value:  14.66 %\n",
      "Scores from CV with HPO [-2295841.32122941 -2355683.14407989 -2325474.41281686 -2250273.59315264\n",
      " -2479044.06077922]\n",
      "Average MAE scores with HPO:  2341263.306411603\n",
      "Average MAE scores with HPO as a percentage of the mean y value:  13.88 %\n",
      "The best number of neighbors when using negative mean absolute error scoring is:  {'n_neighbors': 12}\n",
      "The best MAE score attained was:  2334433.779728959\n",
      "HPO leads to a  5.36 %  improvement in MAE score over the model without HPO\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the cross validation scheme.\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=56)\n",
    "\n",
    "\n",
    "scores = cross_val_score(knn, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "print(scores)\n",
    "print(f\"{'Average MAE scores without HPO: '}\", -scores.mean())\n",
    "print(f\"{'Average MAE scores without HPO as a percentage of the mean y value: '}\", f\"{(-scores.mean()/np.mean(y)*100):.2f}\", '%')\n",
    "\n",
    "\n",
    "# Define the search space\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 50)),\n",
    "}\n",
    "\n",
    "\n",
    "# TODO - think about iterating over the grid and storing results to be able to plot the results. Have to put after fit func.\n",
    "# print( pd.DataFrame(better_knn.cv_results_)[['mean_test_score', 'std_test_score', 'params']].sort_values(by='mean_test_score', ascending=False).head(5) )\n",
    "\n",
    "\n",
    "better_knn = GridSearchCV(knn, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_error', refit=True)\n",
    "\n",
    "#get the scores from the cross validation with HPO\n",
    "\n",
    "\n",
    "better_knn.fit(X_train, y_train) \n",
    "better_scores = cross_val_score(better_knn, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(f\"{'Scores from CV with HPO'}\", better_scores)\n",
    "print(f\"{'Average MAE scores with HPO: '}\", -better_scores.mean())\n",
    "print(f\"{'Average MAE scores with HPO as a percentage of the mean y value: '}\", f\"{(-better_scores.mean()/np.mean(y)*100):.2f}\", '%')\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{'The best number of neighbors when using negative mean absolute error scoring is: '}\", better_knn.best_params_)\n",
    "print(f\"{'The best MAE score attained was: '}\", -better_knn.best_score_)\n",
    "\n",
    "print(f\"{'HPO leads to a '}\", f\"{(1-(-better_scores.mean()/np.mean(y))/(-scores.mean()/np.mean(y)))*100:.2f}\", '%', \" improvement in MAE score over the model without HPO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results it appears that the optimal number of neighbors is 14. Using this new tuned model, we can now evaluate the model using the holdout method and our original test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's use this new model to predict values of the test data and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_predictions = better_knn.predict(X_test)\n",
    "\n",
    "\n",
    "my_scores = []\n",
    "\n",
    "\n",
    "mse_values = [\"MSE\"]\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_dummy = mean_squared_error(y_test, y_pred_dummy)\n",
    "\n",
    "mse_values.append(mse)\n",
    "mse_values.append(mse_dummy)\n",
    "\n",
    "my_scores.append(mse_values)\n",
    "\n",
    "\n",
    "\n",
    "r2_values = [\"R2\"]\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "r2_dummy = r2_score(y_test, y_pred_dummy)\n",
    "\n",
    "r2_values.append(r2)\n",
    "r2_values.append(r2_dummy)\n",
    "\n",
    "my_scores.append(r2_values)\n",
    "\n",
    "\n",
    "\n",
    "mae_values = [\"MAE\"]\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mae_dummy = mean_absolute_error(y_test, y_pred_dummy)\n",
    "\n",
    "mae_values.append(mae)\n",
    "mae_values.append(mae_dummy)\n",
    "\n",
    "my_scores.append(mae_values)\n",
    "\n",
    "\n",
    "print(f\"{'Evaluation method':<25} {'Score':<20} {'Dummy Score':<20} {'Ratio (Score/Dummy Score)':<20}\")\n",
    "\n",
    "for item in my_scores:\n",
    "    ratio = item[1]/item[2] if item[2] != 0 else 0\n",
    "    print(f\"{item[0]:<25} {item[1]:<20.3f} {item[2]:<20.3f} {ratio:<10.3f}\")\n",
    "    \n",
    "    # print the ratio of the mse score over the mean value of y_test\n",
    "print(f\"Ratio of mae over mean value of y: {(mae)/np.mean(y):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prediction_scores = {}\n",
    "n_neighbors = np.arange(2, 60, 1)\n",
    "\n",
    "for neighbors in n_neighbors:\n",
    "    knn = KNeighborsRegressor(n_neighbors=neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    prediction_scores[neighbors] = mean_absolute_error(y_test, y_pred)/mae_dummy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(n_neighbors, prediction_scores.values(), label=\"MAE relative error\")\n",
    "# plt.xlabel(\"Number Of Neighbors\")\n",
    "# plt.ylabel(\"Accuracy as MAE of model/MAE of dummy\")\n",
    "# plt.title(\"KNN: Varying number of Neighbors\")\n",
    "\n",
    "# # highlight the minimum error on the plot as a vertical line\n",
    "\n",
    "# plt.axvline(min(prediction_scores, key=prediction_scores.get), color='r', linestyle='dashed', linewidth=1)\n",
    "# print(\"The minimum error is at: \", min(prediction_scores, key=prediction_scores.get))\n",
    "\n",
    "\n",
    "\n",
    "             \n",
    "# plt.legend()\n",
    "# plt.xlim(0, 63)\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
